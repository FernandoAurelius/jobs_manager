# Generated by Django 5.2 on 2025-08-04 21:40

import hashlib

from django.conf import settings
from django.db import migrations, models


def populate_dedup_hash(apps, schema_editor):
    """Populate dedup_hash for existing JobEvent records."""
    JobEvent = apps.get_model("job", "JobEvent")

    # Only process manual_note events
    events = JobEvent.objects.filter(event_type="manual_note", dedup_hash__isnull=True)

    for event in events:
        # Generate hash using same logic as model
        components = [
            str(event.job_id) if event.job_id else "",
            str(event.staff_id) if event.staff_id else "",
            event.description.strip().lower(),
            event.event_type,
        ]

        hash_input = "|".join(components).encode("utf-8")
        event.dedup_hash = hashlib.md5(hash_input).hexdigest()
        event.save(update_fields=["dedup_hash"])


def cleanup_duplicate_events(apps, schema_editor):
    """Remove duplicate events, keeping the oldest one."""
    JobEvent = apps.get_model("job", "JobEvent")

    # Find duplicate groups by dedup_hash
    from django.db.models import Count, Min

    duplicate_hashes = (
        JobEvent.objects.filter(event_type="manual_note", dedup_hash__isnull=False)
        .values("dedup_hash")
        .annotate(count=Count("id"), oldest_timestamp=Min("timestamp"))
        .filter(count__gt=1)
    )

    deleted_count = 0
    for item in duplicate_hashes:
        # Keep only the oldest event
        duplicates = JobEvent.objects.filter(dedup_hash=item["dedup_hash"]).exclude(
            timestamp=item["oldest_timestamp"]
        )

        deleted_count += duplicates.count()
        duplicates.delete()

    if deleted_count > 0:
        print(f"Cleaned up {deleted_count} duplicate events")


def reverse_populate_dedup_hash(apps, schema_editor):
    """Reverse migration - clear dedup_hash field."""
    JobEvent = apps.get_model("job", "JobEvent")
    JobEvent.objects.update(dedup_hash=None)


class Migration(migrations.Migration):
    dependencies = [
        ("job", "0037_finalize_uuid_transition"),
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.AddField(
            model_name="jobevent",
            name="dedup_hash",
            field=models.CharField(
                blank=True,
                help_text="MD5 hash for deduplication based on job+staff+description+type",
                max_length=64,
                null=True,
            ),
        ),
        migrations.AddIndex(
            model_name="jobevent",
            index=models.Index(
                fields=["job", "-timestamp"], name="jobevent_job_timestamp_idx"
            ),
        ),
        migrations.AddIndex(
            model_name="jobevent",
            index=models.Index(
                fields=["event_type", "-timestamp"], name="jobevent_type_timestamp_idx"
            ),
        ),
        migrations.AddIndex(
            model_name="jobevent",
            index=models.Index(
                fields=["staff", "-timestamp"], name="jobevent_staff_timestamp_idx"
            ),
        ),
        migrations.AddIndex(
            model_name="jobevent",
            index=models.Index(fields=["dedup_hash"], name="jobevent_dedup_hash_idx"),
        ),
        # Populate dedup_hash for existing records
        migrations.RunPython(
            populate_dedup_hash, reverse_code=reverse_populate_dedup_hash
        ),
        # Clean up duplicates before adding constraint
        migrations.RunPython(
            cleanup_duplicate_events, reverse_code=migrations.RunPython.noop
        ),
        # Add constraint after cleanup
        migrations.AddConstraint(
            model_name="jobevent",
            constraint=models.UniqueConstraint(
                condition=models.Q(("event_type", "manual_note")),
                fields=("job", "staff", "event_type", "dedup_hash"),
                name="unique_manual_event_per_user_job",
            ),
        ),
    ]
